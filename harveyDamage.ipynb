{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f68e50d-caef-45ab-ae56-77fe7ef77caa",
   "metadata": {},
   "source": [
    "Part 1: (3 points) Data preprocessing and visualization\r\n",
    "\r\n",
    "You will need to perform data analysis and pre-processing to prepare the images for training. At a minimum, you shoul \r\n",
    "\r\n",
    "Write code to load the data into Python data structures\r\n",
    "\r\n",
    "Investigate the datasets to determine basic attributes of the images\r\n",
    "\r\n",
    "Ensure data is split for training, validation and testing and perform any additional preprocessing (e.g., rescaling, normalization, etc.) so that it can be used for training/evaluation of the neural networks you will build in Part 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "19660fad-8c06-4c65-bff7-0766f074d2fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load - train - test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "90199e4d-404c-42b2-89a4-111b666db733",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'coe379L-sp25/datasets/unit03/Project3/damage'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 20\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m [train_pos, test_pos, train_neg, test_neg]:\n\u001b[1;32m     18\u001b[0m     d\u001b[38;5;241m.\u001b[39mmkdir(parents\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 20\u001b[0m all_damage_file_paths \u001b[38;5;241m=\u001b[39m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcoe379L-sp25/datasets/unit03/Project3/damage\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m all_no_damage_file_paths \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mlistdir(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcoe379L-sp25/datasets/unit03/Project3/no_damage\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# def split_and_copy(src_dir, train_dest, test_dest, split_ratio=0.7):\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m#     files = list(src_dir.glob(\"*\"))\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m#     random.shuffle(files)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# split_and_copy(positive_dir, train_pos, test_pos)\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# split_and_copy(negative_dir, train_neg, test_neg)\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'coe379L-sp25/datasets/unit03/Project3/damage'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import random\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "data_dir = Path(\"datasets/unit03/Project3\")\n",
    "positive_dir = data_dir / \"damage\"\n",
    "negative_dir = data_dir / \"no_damage\"\n",
    "\n",
    "output_dir = Path(\"datasets/unit03/Project3\")\n",
    "train_pos = output_dir / \"train/damage\"\n",
    "test_pos = output_dir / \"test/damage\"\n",
    "train_neg = output_dir / \"train/no_damage\"\n",
    "test_neg = output_dir / \"test/no_damage\"\n",
    "\n",
    "for d in [train_pos, test_pos, train_neg, test_neg]:\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "all_damage_file_paths = os.listdir('coe379L-sp25/datasets/unit03/Project3/damage')\n",
    "all_no_damage_file_paths = os.listdir('coe379L-sp25/datasets/unit03/Project3/no_damage')\n",
    "\n",
    "# def split_and_copy(src_dir, train_dest, test_dest, split_ratio=0.7):\n",
    "#     files = list(src_dir.glob(\"*\"))\n",
    "#     random.shuffle(files)\n",
    "#     split_idx = int(len(files) * split_ratio)\n",
    "#     train_files = files[:split_idx]\n",
    "#     test_files = files[split_idx:]\n",
    "    \n",
    "#     for f in train_files:\n",
    "#         shutil.copy(f, train_dest / f.name)\n",
    "#     for f in test_files:\n",
    "#         shutil.copy(f, test_dest / f.name)\n",
    "\n",
    "# split_and_copy(positive_dir, train_pos, test_pos)\n",
    "# split_and_copy(negative_dir, train_neg, test_neg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b25f403-0588-4d4a-b412-1ac9ba03325f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "train_damage_paths = random.sample(all_damage_file_paths, int(len(all_damage_file_paths)*0.8))\n",
    "print(\"train damage image count: \", len(train_damage_paths))\n",
    "test_damage_paths = [ p for p in all_damage_file_paths if p not in train_damage_paths]\n",
    "print(\"test damage image count: \", len(test_damage_paths))\n",
    "# ensure no overlap:\n",
    "overlap = [p for p in train_damage_paths if p in test_damage_paths]\n",
    "print(\"len of overlap: \", len(overlap))\n",
    "\n",
    "train_no_damage_paths = random.sample(all_no_damage_file_paths, int(len(all_no_damage_file_paths)*0.8))\n",
    "print(\"train no-damage image count: \", len(train_no_damage_paths))\n",
    "test_no_damage_paths = [ p for p in all_no_damage_file_paths if p not in train_no_damage_paths]\n",
    "print(\"test no-damage image count: \", len(test_no_damage_paths))\n",
    "# ensure no overlap:\n",
    "overlap = [p for p in train_no_damage_paths if p in test_no_damage_paths]\n",
    "print(\"len of overlap: \", len(overlap))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eeda238-cca0-4846-8177-8a975a929d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ensure to copy the images to the directories\n",
    "# import shutil\n",
    "# for p in train_damage_paths:\n",
    "#     shutil.copyfile(os.path.join('datasets/unit03/Project3/train/damage', p), os.path.join('coe379L-sp25/datasets/unit03/Project3/damage', p) )\n",
    "\n",
    "# for p in test_damage_paths:\n",
    "#     shutil.copyfile(os.path.join('datasets/unit03/Project3/test/damage', p), os.path.join('coe379L-sp25/datasets/unit03/Project3/damage', p) )\n",
    "\n",
    "# for p in train_no_damage_paths:\n",
    "#     shutil.copyfile(os.path.join('datasets/unit03/Project3/train/no_damage', p), os.path.join('coe379L-sp25/datasets/unit03/Project3/no_damage', p) )\n",
    "\n",
    "# for p in test_no_damage_paths:\n",
    "#     shutil.copyfile(os.path.join('datasets/unit03/Project3/test/no_damage', p), os.path.join('coe379L-sp25/datasets/unit03/Project3/no_damage', p) )\n",
    "\n",
    "# # check counts:\n",
    "# print(\"Files in train/damage: \", len(os.listdir('datasets/unit03/Project3/train/damage')))\n",
    "# print(\"Files in train/no_damage: \", len(os.listdir('datasets/unit03/Project3/train/no_damage')))\n",
    "\n",
    "# print(\"Files in test/damage: \", len(os.listdir('datasets/unit03/Project3/test/damage')))\n",
    "# print(\"Files in test/no_damage: \", len(os.listdir('datasets/unit03/Project3/test/no_damage')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbef9df3-aaf4-4266-b102-2544872e5fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import os\n",
    "\n",
    "# Ensure to copy the images to the directories\n",
    "for p in train_damage_paths:\n",
    "    src = os.path.join('coe379L-sp25/datasets/unit03/Project3/damage', p)\n",
    "    dst = os.path.join('datasets/unit03/Project3/train/damage', p)\n",
    "    #print(src)\n",
    "    #print(dst)\n",
    "\n",
    "    # Skip if the source is a directory (like .ipynb_checkpoints)\n",
    "    if os.path.isdir(src):\n",
    "        continue  # Skip this iteration and move to the next file\n",
    "\n",
    "    # Copy the file if it is not a directory\n",
    "    shutil.copyfile(src, dst)\n",
    "\n",
    "for p in test_damage_paths:\n",
    "    src = os.path.join('coe379L-sp25/datasets/unit03/Project3/damage', p)\n",
    "    dst = os.path.join('datasets/unit03/Project3/test/damage', p)\n",
    "    # Skip if the source is a directory (like .ipynb_checkpoints)\n",
    "    if os.path.isdir(src):\n",
    "        continue  # Skip this iteration and move to the next file\n",
    "\n",
    "    # Copy the file if it is not a directory\n",
    "    shutil.copyfile(src, dst)\n",
    "\n",
    "for p in train_no_damage_paths:\n",
    "    src = os.path.join('coe379L-sp25/datasets/unit03/Project3/no_damage', p)\n",
    "    dst = os.path.join('datasets/unit03/Project3/train/no_damage', p)\n",
    "    # Skip if the source is a directory (like .ipynb_checkpoints)\n",
    "    if os.path.isdir(src):\n",
    "        continue  # Skip this iteration and move to the next file\n",
    "\n",
    "    # Copy the file if it is not a directory\n",
    "    shutil.copyfile(src, dst)\n",
    "\n",
    "for p in test_no_damage_paths:\n",
    "    src = os.path.join('coe379L-sp25/datasets/unit03/Project3/no_damage', p)\n",
    "    dst = os.path.join('datasets/unit03/Project3/test/no_damage', p)\n",
    "    # Skip if the source is a directory (like .ipynb_checkpoints)\n",
    "    if os.path.isdir(src):\n",
    "        continue  # Skip this iteration and move to the next file\n",
    "\n",
    "    # Copy the file if it is not a directory\n",
    "    shutil.copyfile(src, dst)\n",
    "\n",
    "# Check counts:\n",
    "print(\"Files in train/damage: \", len(os.listdir('datasets/unit03/Project3/train/damage')))\n",
    "print(\"Files in train/no_damage: \", len(os.listdir('datasets/unit03/Project3/train/no_damage')))\n",
    "print(\"Files in test/damage: \", len(os.listdir('datasets/unit03/Project3/test/damage')))\n",
    "print(\"Files in test/no_damage: \", len(os.listdir('datasets/unit03/Project3/test/no_damage')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "450fd4a4-a082-4bc0-8675-ed064611cd36",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Total Train\n",
    "print(\"Total Train: \", len(train_damage_paths) + len(train_no_damage_paths))\n",
    "\n",
    "#Total Test\n",
    "print(\" Total Test: \", len(test_damage_paths) + len(test_no_damage_paths))\n",
    "\n",
    "#Total Images\n",
    "print(\" Total: \", len(train_damage_paths) + len(train_no_damage_paths) + len(test_damage_paths) + len(test_no_damage_paths))\n",
    "\n",
    "#Ratio\n",
    "print(\" Test Ratio: \", (len(test_damage_paths) + len(test_no_damage_paths) )/ 21323)\n",
    "print(\" Test Ratio: \", (len(train_damage_paths) + len(train_no_damage_paths) )/ 21323)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b3e4ddb-e5b3-4c70-ad07-25baa4180e83",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#image attributes: \n",
    "\n",
    "# Size (width, height): (128, 128)\n",
    "# Mode: RGB\n",
    "# Format: JPEG\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "damage_folder = Path(\"coe379L-sp25/datasets/unit03/Project3/damage\")\n",
    "\n",
    "image_files = list(damage_folder.glob(\"*.[jJpP]*[gGnN]*\"))  # catches jpg, jpeg, png, etc.\n",
    "\n",
    "\n",
    "img_path = random.choice(image_files)\n",
    "\n",
    "print(\"Randomly selected image:\", img_path)\n",
    "img = Image.open(img_path)\n",
    "\n",
    "plt.imshow(img)\n",
    "plt.axis(\"off\")  # hides axes\n",
    "plt.title(\"Damage Example\")\n",
    "plt.show()\n",
    "\n",
    "print(\"Size (width, height):\", img.size)\n",
    "print(\"Mode:\", img.mode)\n",
    "print(\"Format:\", img.format)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da4949c-b58b-467b-86b0-6f3551963584",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # rescaling / normalization\n",
    "\n",
    "# from PIL import Image\n",
    "# from pathlib import Path\n",
    "# import numpy as np\n",
    "\n",
    "# # Input dataset path\n",
    "# base_path = Path(\"datasets/unit03/Project3\")  # Adjust this to your structure\n",
    "\n",
    "# # Output path for normalized images\n",
    "# # ??\n",
    "# normalized_path = Path(\"preDatasets/unit03/Project3\")\n",
    "# normalized_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# # Folder splits and class names\n",
    "# splits = ['train', 'test']\n",
    "# classes = ['damage', 'no_damage']\n",
    "\n",
    "# for split in splits:\n",
    "#     for cls in classes:\n",
    "#         src_folder = base_path / split / cls\n",
    "#         dest_folder = normalized_path / split / cls\n",
    "#         dest_folder.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "#         img_files = list(src_folder.glob(\"*.*\"))\n",
    "\n",
    "#         for img_path in img_files:\n",
    "#             try:\n",
    "#                 # Load image in RGB\n",
    "#                 img = Image.open(img_path).convert(\"RGB\")\n",
    "                \n",
    "#                 # Normalize image (scale to [0, 1]) and back to [0, 255] uint8\n",
    "#                 img_array = np.array(img) / 255.0\n",
    "#                 normalized_array = (img_array * 255).astype(np.uint8)\n",
    "#                 normalized_img = Image.fromarray(normalized_array)\n",
    "\n",
    "#                 # Save normalized image with same filename\n",
    "#                 out_path = dest_folder / img_path.name\n",
    "#                 normalized_img.save(out_path)\n",
    "\n",
    "#             except Exception as e:\n",
    "#                 print(f\"Error processing {img_path.name}: {e}\")\n",
    "\n",
    "\n",
    "# #Total Train\n",
    "# print(\"Total Train: \", len(preDatasets/unit03/Project3) )\n",
    "\n",
    "# #Total Test\n",
    "# print(\" Total Test: \", len(test_damage_paths) + len(test_no_damage_paths))\n",
    "\n",
    "# #Total Images\n",
    "# print(\" Total: \", len(train_damage_paths) + len(train_no_damage_paths) + len(test_damage_paths) + len(test_no_damage_paths))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e977a916-f470-49dc-a685-7919ea66d576",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import needed classes and functions\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "#from tensorflow.keras.layers.experimental.preprocessing import Rescaling\n",
    "#from tensorflow.keras.layers.preprocessing import Rescaling\n",
    "from tensorflow.keras.layers import Rescaling\n",
    "\n",
    "\n",
    "\n",
    "# path to training data\n",
    "train_data_dir = 'datasets/unit03/Project3/train' # select train folder <-------------------------------------\n",
    "\n",
    "# controls the size of the \"batches\" of images streamed when accessing the datasets.\n",
    "# this is useful to control the memory usage with very large datasets\n",
    "batch_size = 32\n",
    "\n",
    "# target image size\n",
    "img_height = 128\n",
    "img_width = 128\n",
    "\n",
    "# note that the subset parameter can take values of \"training\", \"validation\", or \"both\";\n",
    "# the value dictates which dataset is returned (we want both)\n",
    "train_ds, val_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    train_data_dir,\n",
    "    validation_split=0.2,\n",
    "    subset=\"both\",\n",
    "    seed=123,\n",
    "    image_size=(img_height, img_width),\n",
    "    batch_size=batch_size\n",
    ")\n",
    "# rescale instance\n",
    "rescale = Rescaling(scale=1.0/255)\n",
    "\n",
    "# apply the rescale to the train and validation sets\n",
    "train_rescale_ds = train_ds.map(lambda image,label:(rescale(image),label))\n",
    "val_rescale_ds = val_ds.map(lambda image,label:(rescale(image),label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "474dc0b6-f480-46f3-8bdf-e86539e6f6f5",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m test_data_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdatasets/unit03/Project3/test\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;66;03m# select test folder <-------------------------------------\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# we do not set subset=both here because we do not want the test set split\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m test_ds \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mimage_dataset_from_directory(\n\u001b[1;32m      6\u001b[0m     test_data_dir,\n\u001b[1;32m      7\u001b[0m     seed\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m123\u001b[39m,\n\u001b[1;32m      8\u001b[0m     image_size\u001b[38;5;241m=\u001b[39m(img_height, img_width),\n\u001b[1;32m      9\u001b[0m )\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# approach 1: manually rescale data --\u001b[39;00m\n\u001b[1;32m     12\u001b[0m rescale \u001b[38;5;241m=\u001b[39m Rescaling(scale\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m255\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tf' is not defined"
     ]
    }
   ],
   "source": [
    "# path to test data\n",
    "test_data_dir = 'datasets/unit03/Project3/test' # select test folder <-------------------------------------\n",
    "\n",
    "# we do not set subset=both here because we do not want the test set split\n",
    "test_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    test_data_dir,\n",
    "    seed=123,\n",
    "    image_size=(img_height, img_width),\n",
    ")\n",
    "\n",
    "# approach 1: manually rescale data --\n",
    "rescale = Rescaling(scale=1.0/255)\n",
    "test_rescale_ds = test_ds.map(lambda image,label:(rescale(image),label))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee4dc1e-ade3-4942-b0cb-2425e9df423c",
   "metadata": {},
   "source": [
    "Part 2: (10 points) Model design, training and evaluation\r\n",
    "\r\n",
    "You will explore different model architectures that we have seen in class, including:\r\n",
    "\r\n",
    "A dense (i.e., fully connected) ANN\r\n",
    "\r\n",
    "The Lenet-5 CNN architecture\r\n",
    "\r\n",
    "Alternate-Lenet-5 CNN architecture, described in the following paper (Table 1, Page 12 of the research paper https://arxiv.org/pdf/1807.01688.pdf, but note that the dataset is not the same as that analyzed in thed in the paper.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3e89ffd7-ebfc-406f-a335-81ae10449d74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)   │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,792</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │        <span style=\"color: #00af00; text-decoration-color: #00af00\">18,464</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │         <span style=\"color: #00af00; text-decoration-color: #00af00\">9,248</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8192</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">819,300</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">84</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">8,484</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">170</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv2d_2 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m64\u001b[0m)   │         \u001b[38;5;34m1,792\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_2 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_3 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │        \u001b[38;5;34m18,464\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_3 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_4 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │         \u001b[38;5;34m9,248\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_4 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_1 (\u001b[38;5;33mFlatten\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8192\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)            │       \u001b[38;5;34m819,300\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_4 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m84\u001b[0m)             │         \u001b[38;5;34m8,484\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_5 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)              │           \u001b[38;5;34m170\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">857,458</span> (3.27 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m857,458\u001b[0m (3.27 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">857,458</span> (3.27 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m857,458\u001b[0m (3.27 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# dense - any # of layers\n",
    "\n",
    "from keras import layers\n",
    "from keras import models\n",
    "import pandas as pd\n",
    "from keras import optimizers\n",
    "\n",
    "# Intializing a sequential model\n",
    "model_cnn = models.Sequential()\n",
    "\n",
    "# Adding first conv layer with 64 filters and kernel size 3x3 ,\n",
    "# Recall: using padding='same' ensures the output size has the same shape as the input size\n",
    "model_cnn.add(layers.Conv2D(64, (3, 3), activation='relu', padding=\"same\", input_shape=(128,128,3))) #First COnv2D Layer\n",
    "\n",
    "# Adding max pooling to reduce the size of output of first conv layer\n",
    "model_cnn.add(layers.MaxPooling2D((2, 2), padding='same'))\n",
    "\n",
    "model_cnn.add(layers.Conv2D(32, (3, 3), activation='relu', padding=\"same\"))\n",
    "model_cnn.add(layers.MaxPooling2D((2, 2), padding='same'))\n",
    "\n",
    "model_cnn.add(layers.Conv2D(32, (3, 3), activation='relu', padding=\"same\"))\n",
    "model_cnn.add(layers.MaxPooling2D((2, 2), padding='same')) #MaxPooling 2D layer\n",
    "\n",
    "# flattening the output of the conv layer after max pooling to make it ready for creating dense connections\n",
    "model_cnn.add(layers.Flatten())\n",
    "\n",
    "# Adding a fully connected dense layer with 100 neurons\n",
    "model_cnn.add(layers.Dense(100, activation='relu'))\n",
    "\n",
    "# Adding a fully connected dense layer with 84 neurons\n",
    "model_cnn.add(layers.Dense(84, activation='relu'))\n",
    "\n",
    "# Adding the output layer with * neurons and activation functions as softmax since this is a multi-class classification problem\n",
    "model_cnn.add(layers.Dense(2, activation='softmax')) #MaxPooling Output Layer - dam/nodam\n",
    "\n",
    "# Compile model\n",
    "# RMSprop (Root Mean Square Propagation) is commonly used in training deep neural networks.\n",
    "model_cnn.compile(optimizer=optimizers.RMSprop(learning_rate=1e-4), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Generating the summary of the model\n",
    "model_cnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a4a5c7dd-ee28-46f6-8963-67051abe1b86",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_rescale_ds' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# fit the model from image generator\u001b[39;00m\n\u001b[1;32m      2\u001b[0m history \u001b[38;5;241m=\u001b[39m model_cnn\u001b[38;5;241m.\u001b[39mfit(\n\u001b[0;32m----> 3\u001b[0m             \u001b[43mtrain_rescale_ds\u001b[49m,\n\u001b[1;32m      4\u001b[0m             batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m,\n\u001b[1;32m      5\u001b[0m             epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m,\n\u001b[1;32m      6\u001b[0m             validation_data\u001b[38;5;241m=\u001b[39mval_rescale_ds\n\u001b[1;32m      7\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_rescale_ds' is not defined"
     ]
    }
   ],
   "source": [
    "# fit the model from image generator\n",
    "history = model_cnn.fit(\n",
    "            train_rescale_ds,\n",
    "            batch_size=32,\n",
    "            epochs=20,\n",
    "            validation_data=val_rescale_ds\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d335d955-36aa-42b9-b3fa-582eb49c58fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_accuracy = model_cnn.evaluate(test_rescale_ds, verbose=0)\n",
    "test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5712c54-eabf-4c0b-874b-60cbe171bfe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lenet - 5 - can follow class example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91602b7a-3a60-4a64-b212-bcfa964e9b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import layers\n",
    "from keras import models\n",
    "import pandas as pd\n",
    "\n",
    "model_lenet5 = models.Sequential()\n",
    "\n",
    "# Layer 1: Convolutional layer with 6 filters of size 3x3, followed by average pooling\n",
    "model_lenet5.add(layers.Conv2D(6, kernel_size=(3, 3), activation='relu', input_shape=(128,128,3)))\n",
    "model_lenet5.add(layers.AveragePooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# Layer 2: Convolutional layer with 16 filters of size 3x3, followed by average pooling\n",
    "model_lenet5.add(layers.Conv2D(16, kernel_size=(3, 3), activation='relu'))\n",
    "model_lenet5.add(layers.AveragePooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# Flatten the feature maps to feed into fully connected layers\n",
    "# which layer\n",
    "model_lenet5.add(layers.Flatten())\n",
    "\n",
    "# Layer 3: Fully connected layer with 120 neurons\n",
    "model_lenet5.add(layers.Dense(120, activation='relu'))\n",
    "\n",
    "# Layer 4: Fully connected layer with 84 neurons\n",
    "model_lenet5.add(layers.Dense(84, activation='relu'))\n",
    "\n",
    "# Output layer: Fully connected layer with num_classes neurons (e.g., 3 )\n",
    "model_lenet5.add(layers.Dense(3, activation='softmax'))\n",
    "\n",
    "# Compile model\n",
    "model_lenet5.compile(optimizer=optimizers.RMSprop(learning_rate=1e-4), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Generating the summary of the model\n",
    "model_lenet5.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c7db40b-5dd2-427a-8e14-b7703e900126",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the model from image generator\n",
    "history = model_lenet5.fit(\n",
    "            train_rescale_ds,\n",
    "            batch_size=32,\n",
    "            epochs=20,\n",
    "            validation_data=val_rescale_ds\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c021854-8543-4b3d-bfab-65747b90ae0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_accuracy = model_lenet5.evaluate(test_rescale_ds, verbose=0)\n",
    "test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eec3620-8118-4360-8b61-fffff59e6991",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternate - Lenet - 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f78b576-b103-42e1-88f9-170d94f49de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import layers, models, optimizers\n",
    "\n",
    "model_lenet5 = models.Sequential()\n",
    "\n",
    "# Layer 1: Convolutional layer with 32 filters of size 3x3 (modified from 6 filters)\n",
    "model_lenet5.add(layers.Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(128, 128, 3)))  # Modified: Increased filters\n",
    "model_lenet5.add(layers.MaxPooling2D(pool_size=(2, 2)))  # Modified: Changed average pooling to max pooling\n",
    "\n",
    "# Layer 2: Convolutional layer with 64 filters of size 3x3 (modified from 16 filters)\n",
    "model_lenet5.add(layers.Conv2D(64, kernel_size=(3, 3), activation='relu'))  # Modified: Increased filters\n",
    "model_lenet5.add(layers.MaxPooling2D(pool_size=(2, 2)))  # Modified: Changed average pooling to max pooling\n",
    "\n",
    "# Flatten the feature maps to feed into fully connected layers\n",
    "model_lenet5.add(layers.Flatten())\n",
    "\n",
    "# Layer 3: Fully connected layer with 256 neurons (modified from 120 neurons)\n",
    "model_lenet5.add(layers.Dense(256, activation='relu'))  # Modified: Increased number of neurons\n",
    "\n",
    "# Layer 4: Fully connected layer with 128 neurons (modified from 84 neurons)\n",
    "model_lenet5.add(layers.Dense(128, activation='relu'))  # Modified: Increased number of neurons\n",
    "\n",
    "# Output layer: Fully connected layer with 1 neuron (for binary classification, sigmoid activation)\n",
    "# If multi-class classification, change to the number of classes (e.g., 3 or more)\n",
    "model_lenet5.add(layers.Dense(1, activation='sigmoid'))  # Modified: Binary classification with sigmoid activation\n",
    "\n",
    "# Compile model\n",
    "model_lenet5.compile(optimizer=optimizers.RMSprop(learning_rate=1e-4), loss='binary_crossentropy', metrics=['accuracy'])  # Modified loss for binary classification\n",
    "\n",
    "# Generating the summary of the model\n",
    "model_lenet5.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d5f4592-0312-42df-b65a-5d6f305d3f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "# from tensorflow.keras import layers, models, optimizers\n",
    "# from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# # Paths to your train and test directories\n",
    "# train_data_dir = 'path_to_train_data'  # Train data path (should have 'damaged' and 'undamaged' subdirectories) ---------\n",
    "# test_data_dir = 'path_to_test_data'    # Test data path (should have 'damaged' and 'undamaged' subdirectories) -----------\n",
    "\n",
    "\n",
    "# # Data Augmentation and Preprocessing for already scaled data\n",
    "# train_datagen = ImageDataGenerator(\n",
    "#     # No rescaling, assuming data is already in [0, 1]\n",
    "#     rotation_range=40,  # Random rotations for augmentation\n",
    "#     width_shift_range=0.2,  # Horizontal shifts\n",
    "#     height_shift_range=0.2,  # Vertical shifts\n",
    "#     shear_range=0.2,  # Shear transformations\n",
    "#     zoom_range=0.2,  # Zoom in/out\n",
    "#     horizontal_flip=True,  # Randomly flip images horizontally\n",
    "#     fill_mode='nearest'  # Fill in missing pixels from transformations\n",
    "# )\n",
    "\n",
    "# test_datagen = ImageDataGenerator()  # No rescaling, assuming test data is also [0, 1]\n",
    "\n",
    "# # Load the training and test datasets using the flow_from_directory method\n",
    "# train_generator = train_datagen.flow_from_directory(\n",
    "#     train_data_dir,\n",
    "#     target_size=(128, 128),  # Resize images to 128x128\n",
    "#     batch_size=32,\n",
    "#     class_mode='binary'  # Binary classification (damaged vs. undamaged)\n",
    "# )\n",
    "\n",
    "# test_generator = test_datagen.flow_from_directory(\n",
    "#     test_data_dir,\n",
    "#     target_size=(128, 128),  # Resize images to 128x128\n",
    "#     batch_size=32,\n",
    "#     class_mode='binary'  # Binary classification\n",
    "# )\n",
    "\n",
    "\n",
    "# # Model Architecture: Modified LeNet-5 Approach\n",
    "# model_cnn = models.Sequential()\n",
    "\n",
    "# # First Convolutional Layer: 32 filters, 5x5 kernel, ReLU activation\n",
    "# model_cnn.add(layers.Conv2D(32, (5, 5), activation='relu', padding='same', input_shape=(128, 128, 3)))\n",
    "\n",
    "# # MaxPooling Layer: 2x2 pool size\n",
    "# model_cnn.add(layers.MaxPooling2D((2, 2), padding='same'))\n",
    "\n",
    "# # Second Convolutional Layer: 64 filters, 5x5 kernel, ReLU activation\n",
    "# model_cnn.add(layers.Conv2D(64, (5, 5), activation='relu', padding='same'))\n",
    "\n",
    "# # MaxPooling Layer: 2x2 pool size\n",
    "# model_cnn.add(layers.MaxPooling2D((2, 2), padding='same'))\n",
    "\n",
    "# # Flatten the output for the fully connected layer\n",
    "# model_cnn.add(layers.Flatten())\n",
    "\n",
    "# # Fully Connected Layer: 120 neurons, ReLU activation\n",
    "# model_cnn.add(layers.Dense(120, activation='relu'))\n",
    "\n",
    "# # Fully Connected Layer: 84 neurons, ReLU activation\n",
    "# model_cnn.add(layers.Dense(84, activation='relu'))\n",
    "\n",
    "# # Output Layer: 1 neuron (for binary classification), sigmoid activation\n",
    "# model_cnn.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "# # Compile the model\n",
    "# model_cnn.compile(optimizer=optimizers.RMSprop(learning_rate=1e-4),\n",
    "#                   loss='binary_crossentropy',  # Binary classification loss\n",
    "#                   metrics=['accuracy'])\n",
    "\n",
    "# # Model Summary\n",
    "# model_cnn.summary()\n",
    "\n",
    "# # Train the model\n",
    "# history = model_cnn.fit(\n",
    "#     train_generator,  # Training data generator\n",
    "#     steps_per_epoch=train_generator.samples // train_generator.batch_size,  # Steps per epoch\n",
    "#     epochs=20,  # Number of epochs\n",
    "#     validation_data=test_generator,  # Validation data generator\n",
    "#     validation_steps=test_generator.samples // test_generator.batch_size  # Validation steps\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc8f3bb-a516-4541-b6fc-fab3e9ff8431",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the model from image generator\n",
    "history = model_lenet5.fit(\n",
    "            train_rescale_ds,\n",
    "            batch_size=32,\n",
    "            epochs=20,\n",
    "            validation_data=val_rescale_ds\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3382ce2f-3fb2-4006-871a-7c0b6f5db98d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_accuracy = model_lenet5.evaluate(test_rescale_ds, verbose=0)\n",
    "test_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e214211-5c5e-4356-b9ad-aeea07edb04a",
   "metadata": {},
   "source": [
    "Part 3: (7 points) Model inference server and deployment **help class site MLOps -> developing an inference server in flask\r\n",
    "\r\n",
    "For the best model built in part 2, persist the trained model to disk so that it can be reconstituted easily. Develop a simple inference server to serve your trained model over HTTP. There should be at least two endpoints:\r\n",
    "\r\n",
    "A model summary endpoint GET /summary providing metadata about the model.\r\n",
    "\r\n",
    "Note: This endpoint must be accept requests to: GET /summary and it must return a JSON response.\r\n",
    "\r\n",
    "An inference endpoint POST /inference that can perform classification on a\n",
    "\n",
    "#cannot build docker image in jupyter server (use command line on VM)\n",
    "everything in your notebook should be on machine in nb-data\n",
    "\n",
    "Dockerfile and api.py can be created in notebook but docker build and docker commands (run) cant be in jupter\n",
    "\n",
    "\n",
    "unit 3 for project have to do option 1\n",
    "1/ Requre the user send a raw image file, such as a png or jpgn image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ea6969-638a-48f9-b46f-5d2713029b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model\n",
    "# Save the model in the new .keras format\n",
    "model_lenet5.save('b_model.keras')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33d4bd44-bf58-46fa-96f7-d4837fbb4396",
   "metadata": {},
   "source": [
    "Part 4: (7 points) Write a 3 page report summarizing your work. (info on class page)) (1 pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "786e0c1e-fe27-4b08-9114-c131a5f7a390",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "#model = tf.keras.models.load_model('b_model.keras')\n",
    "\n",
    "##model.load(\"b_mode.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d44bce-01d2-462b-9f5f-20b92c8ab94b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
